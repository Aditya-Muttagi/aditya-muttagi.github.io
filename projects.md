---
layout: page
title: "Projects"
permalink: /projects/
---

<section class="page-hero fade-in-up">
  <p class="hero-kicker">CASE STUDIES</p>
  <h1>Turning questions into decisions</h1>
  <p class="hero-subtitle narrow">
    A few examples of how I approach product and data problems — from scoping the question,
    to structuring the analysis, to turning it into a narrative stakeholders can act on.
  </p>
</section>

<section class="section-block fade-in-up delay-1" id="onboarding-funnel">
  <article class="project-detail-card">
    <p class="project-tag">Funnel analysis · Onboarding</p>
    <h2>Reducing drop-offs in a key onboarding flow</h2>

    <div class="project-columns">
      <div>
        <h3>The context</h3>
        <p>
          A multi-step onboarding flow was seeing good top-of-funnel traffic,
          but poor completion. Multiple teams had different hypotheses, but no
          shared view of where exactly users were dropping.
        </p>

        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Mapped the complete user journey with clear event definitions</li>
          <li>Built a stepwise funnel view and quantified drop-offs at each stage</li>
          <li>Segmented by device type and traffic source to uncover patterns</li>
          <li>Created a single slide that captured the “true” problem statement</li>
        </ul>
      </div>

      <div>
        <h3>How I approached it</h3>
        <ul class="bullet-list">
          <li>Focused first on a trustworthy funnel definition and event quality</li>
          <li>Resisted over-segmentation until the core picture was stable</li>
          <li>Turned scattered hypotheses into a prioritised list of UX changes</li>
        </ul>

        <h3>Outcome</h3>
        <p>
          The team aligned on two high-friction steps to fix first, instead of
          debating personal intuitions. Follow-up experiments were scoped with
          realistic expectations on potential impact.
        </p>

        <ul class="project-meta">
          <li>Tools: SQL / BI tool funnel views</li>
          <li>Output: diagnostic deck + lightweight monitoring view</li>
        </ul>
      </div>
    </div>
  </article>
</section>

<section class="section-block fade-in-up delay-2" id="retention-cohorts">
  <article class="project-detail-card">
    <p class="project-tag">Retention · Cohorts</p>
    <h2>Understanding what drives repeat usage</h2>

    <div class="project-columns">
      <div>
        <h3>The context</h3>
        <p>
          Leadership wanted to “improve retention”, but the product had no clear 
          view of who was actually retained, and what early behaviours predicted it.
        </p>

        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Defined retention clearly (and what counts as meaningful activity)</li>
          <li>Built weekly cohorts and plotted retention curves over time</li>
          <li>Segmented cohorts by acquisition channel and early usage patterns</li>
          <li>Identified behaviours strongly associated with better long-term use</li>
        </ul>
      </div>

      <div>
        <h3>How I approached it</h3>
        <ul class="bullet-list">
          <li>Started from business reality: what behaviour actually matters</li>
          <li>Made trade-offs explicit (e.g., engagement vs. pure activity).</li>
          <li>Annotated charts with plain language explanations for stakeholders</li>
        </ul>

        <h3>Outcome</h3>
        <p>
          The team refined its activation metric, focused onboarding on a small
          set of “healthy behaviours”, and had a much clearer baseline for
          evaluating retention experiments.
        </p>

        <ul class="project-meta">
          <li>Methods: cohort analysis, retention curves</li>
          <li>Output: annotated visuals + short write-up</li>
        </ul>
      </div>
    </div>
  </article>
</section>

<section class="section-block fade-in-up delay-3" id="executive-dashboard">
  <article class="project-detail-card">
    <p class="project-tag">Reporting · Metric design</p>
    <h2>From scattered reports to a single, trusted view</h2>

    <div class="project-columns">
      <div>
        <h3>The context</h3>
        <p>
          Stakeholders were receiving multiple reports with different metrics,
          definitions, and time windows. As a result, meetings spent more time
          reconciling numbers than making decisions.
        </p>

        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Audited existing reports and reconciled metric definitions</li>
          <li>Agreed on a concise set of core product and operational metrics</li>
          <li>Designed a single weekly “source of truth” view</li>
          <li>Added commentary templates to capture context and caveats</li>
        </ul>
      </div>

      <div>
        <h3>How I approached it</h3>
        <ul class="bullet-list">
          <li>Prioritised clarity and consistency over volume of metrics</li>
          <li>Worked closely with stakeholders on what they actually use</li>
          <li>Ensured the view could be maintained with minimal friction</li>
        </ul>

        <h3>Outcome</h3>
        <p>
          Leadership had a single, reliable file to review each week.
          Decision time shortened, and ad-hoc number disputes dropped sharply.
        </p>

        <ul class="project-meta">
          <li>Focus: definition alignment, stakeholder communication</li>
          <li>Output: weekly dashboard + metric documentation</li>
        </ul>
      </div>
    </div>
  </article>
</section>
